{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1/ Scrap-booking..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'objectif de cette première étape est de scraper les hôtels de la liste des 35 destinations.\n",
    "Le spider a été généré initialement via scrapy en mode framework (voir dossier 'scrapbooking').\n",
    "Il est reproduit ici en mode 'notebook' et fonctionne tout autant (à l'heure où nous écrivons ces lignes, en espérant que le site de booking n'aura pas trop évolué pour que les xpath des éléments soient les même...).\n",
    "\n",
    "Après étude, il a été choisi de boucler sur les destinations en tentant de remplir le champs de recherche de la page booking, à chaque fois, et non pas en modifiant l'url, car cette deuxième solution suppose de partir d'une très longue url comme url de base, et d'itérer grâce à un paramètre '&ss=' inclu au milieu de celle-ci. La manoeuvre paraît assez hasardeuse, et peut-être moins stable sur le long terme que la première méthode qui utilise le champs de recherche.\n",
    "\n",
    "En revanche, l'url est modifiée, après chaque 'atterissage' sur une page de listes d'hôtels pour ne sélectionner que les hôtels à proprement parler, en ajoutant un '&nflt=ht_id%3D204' (un filtre additionnel de booking) à l'url en place. Ici, cela présente moins de risque car on part d'une url de base entière et on travaille uniquement sur un terminaison optionnelle pour booking.\n",
    "\n",
    "Seules les 25 premiers hôtels de la première page (filtrées en hôtels et apart'hôtels) sont scrappés.\n",
    "En effet, selon nous, scraper toutes les pages est inutile (et énergivore) sauf si nous souhaitons prendre un panel beaucoup plus large pour appliquer nos propres critères de sélections des hôtels... Mais nos critères, aussi sophistiqués soient-ils, seront-ils les mêmes que les clients ? Aura-t-il possibilité de faire varier ces critères ? Non, car l'objectif est de promouvoir des destinations avant tout, et ensuite de proposer un petit panel d'hôtel pour ces destinations... pour une sorte de petit widget typiquement disposé sur une page d'accueil ou une page ressource du site Kayak.\n",
    "Ainsi, nous faisons confiance à Booking (et nos équipe de collègues !) pour avoir sélectionner en première page les hôtels qu'il veut recommander en priorité (le filtre d'ordre d'affichage par défaut est \"Nos préférés\"). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "import logging\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importation de la liste des 35 destinations :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_src = pd.read_csv('src/top35_list_cities.txt').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_cities = list_src['Cities'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Génération du spider :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - SPIDER -\n",
    "class BookingSpider(scrapy.Spider):\n",
    "    name = 'scrap-booking'\n",
    "    allowed_domains = ['www.booking.com']\n",
    "    start_urls = ['https://www.booking.com/index.fr.html/']\n",
    "\n",
    "    \n",
    "    def parse(self, response):\n",
    "        # fonction parse d'entrée sur le site - on remplit la première destination de la liste\n",
    "        # (pour nous, dans ce cas, il s'agit d'une fonction qui ne sera utilisée qu'une fois)\n",
    "        iter=0\n",
    "        return scrapy.FormRequest.from_response(\n",
    "                response,\n",
    "                formdata={'ss': list_cities[0]}, # - on utilise le champs 'ss' pour remplir le champs avec la première destination\n",
    "                callback=self.change_url_hostels, \n",
    "                meta={'iter':iter} # - on définit une un itérable à 0, itérateur sur les destinations de la liste\n",
    "            )\n",
    "    \n",
    "\n",
    "    def change_url_hostels(self, response):\n",
    "        # fonction appelée dès qu'on atterit dans une nouvelle page de destination\n",
    "        # objectif : changer l'url pour ne visualiser que des hôtels\n",
    "\n",
    "        iter=response.request.meta['iter'] # on récupère notre variable méta : l'itérable\n",
    "\n",
    "        url_dest=response.url # on récupère l'url en place\n",
    "        yield response.follow(url_dest + '&nflt=ht_id%3D204', # on ajoute à l'url en place la variable qui filtre les hotels\n",
    "                                callback=self.parse_my_dest, \n",
    "                                meta={'iter':iter})\n",
    "\n",
    "    \n",
    "    def parse_my_dest(self, response):\n",
    "        # fonction appelée sur une page de city avec la liste (exclusivement) des hôtels\n",
    "\n",
    "        iter=response.request.meta['iter'] # on récupère notre itérable dans le champs méta\n",
    "\n",
    "        list_hostels_urls=response.xpath('//*[@id=\"search_results_table\"]/div[2]/div/div/div/div[3]/div[*]/div[1]/div[2]/div/div[1]/div[1]/div/div[1]/div/h3/a/@href').getall()\n",
    "        #on scrap la liste des 25 urls d'hotels de la page en une seule fois, grâce à l' * et à getall()\n",
    "\n",
    "        for i, link in enumerate(list_hostels_urls):\n",
    "            # pour chaque lien/hotel de la page, on appelle une fonction de scraop dans la page de l'hotel\n",
    "            yield response.follow(link,\n",
    "                                callback=self.in_hostel_page,\n",
    "                                meta={'city':list_cities[iter],'hostel_url':link}) # transport avec meta du nom de la ville et du lien de l'hotel\n",
    "\n",
    "        iter+=1\n",
    "        if iter<=len(list_cities)-1:\n",
    "        # quand les 25 links/hotels sont scrappés, on itère notre variable en vue de la prochaine destination\n",
    "        # la prochaine destination est rempli dans le Form de la page en cours\n",
    "            yield scrapy.FormRequest.from_response(\n",
    "                response,\n",
    "                formdata={'ss': list_cities[iter]},\n",
    "                callback=self.change_url_hostels,\n",
    "                meta={'iter':iter} # transport avec méta de l'itérable\n",
    "            )\n",
    "\n",
    "    def in_hostel_page(self,response):\n",
    "    # fonction appelée quand on atterit sur une page propre à l'hotel\n",
    "\n",
    "        # redéploiement des variables méta - qui feront parties du yield final\n",
    "        city=response.request.meta['city']\n",
    "        hostel_url=response.request.meta['hostel_url']\n",
    "\n",
    "        # scrapping de toutes les infos recherchées\n",
    "        hostel_name=response.xpath('//div[@id=\"hp_hotel_name\"]/div/div/h2/text()').get()\n",
    "        hostel_img=response.xpath('//div[@id=\"hotel_main_content\"]/div/div/div[3]/a/img/@src').get()\n",
    "        hostel_alt_img=response.xpath('//div[@id=\"hotel_main_content\"]/div/div/div[3]/a/img/@alt').get()\n",
    "        hostel_review=response.xpath('//div[@data-testid=\"review-score-right-component\"]/div/text()').get()\n",
    "        type=response.xpath('//span[@class=\\'e2f34d59b1\\']/text()').get()\n",
    "        lat_long=response.xpath('//a[@id=\\'hotel_surroundings\\']/@data-atlas-latlng').get()\n",
    "        description=response.xpath('//*[@id=\\'property_description_content\\']/p[2]/text()').get()\n",
    "        # ici on compte le nombre d'étoiles ou de carrés de l'hôtel, pour un éventuel classement\n",
    "        nb_squares=len(response.xpath('//*[@data-testid=\\'rating-squares\\']/span[*]').getall())\n",
    "        nb_stars=len(response.xpath('//*[@data-testid=\\'rating-stars\\']/span[*]').getall())\n",
    "        \n",
    "        # yield final\n",
    "        yield {\n",
    "                'city':city,\n",
    "                'hostel_name':hostel_name,\n",
    "                'hostel_url':hostel_url,\n",
    "                'hostel_img':hostel_img,\n",
    "                'hostel_alt_img':hostel_alt_img,\n",
    "                'hostel_review':hostel_review,\n",
    "                'nb_squares':nb_squares,\n",
    "                'nb_stars':nb_stars,\n",
    "                'hostel_type':type,\n",
    "                'lat_long':lat_long,\n",
    "                'description':description\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paramètres pour écraser le fichier s'il est déjà présent :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-14 17:46:00 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-10-14 17:46:00 [scrapy.utils.log] INFO: Versions: lxml 4.8.0.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.2.0, Python 3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1q  5 Jul 2022), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n"
     ]
    }
   ],
   "source": [
    "filename = \"hostels_booking2.csv\"\n",
    "\n",
    "# overwrite du fichier 'result' (si existant), pour un nouveau\n",
    "if filename in os.listdir('results/'):\n",
    "        os.remove('results/' + filename)\n",
    "\n",
    "\n",
    "# # - SETTINGS -\n",
    "process = CrawlerProcess(settings = {\n",
    "    'USER_AGENT': 'Chrome/97.0',\n",
    "    'LOG_LEVEL': logging.INFO,\n",
    "    \"AUTOTHROTTLE_ENABLED\": True,\n",
    "    \"FEEDS\": {\n",
    "        'results/' + filename: {\"format\": \"csv\"}, # récupération du fichier\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lancement du spider et scraping :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-14 17:46:03 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True, 'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-10-14 17:46:03 [scrapy.extensions.telnet] INFO: Telnet Password: 2275df5d98d07d9b\n",
      "2022-10-14 17:46:03 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats',\n",
      " 'scrapy.extensions.throttle.AutoThrottle']\n",
      "2022-10-14 17:46:03 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-10-14 17:46:03 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-10-14 17:46:03 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-10-14 17:46:03 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-10-14 17:46:04 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-10-14 17:46:04 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-10-14 17:47:04 [scrapy.extensions.logstats] INFO: Crawled 39 pages (at 39 pages/min), scraped 33 items (at 33 items/min)\n",
      "2022-10-14 17:48:04 [scrapy.extensions.logstats] INFO: Crawled 90 pages (at 51 pages/min), scraped 80 items (at 47 items/min)\n",
      "2022-10-14 17:49:04 [scrapy.extensions.logstats] INFO: Crawled 143 pages (at 53 pages/min), scraped 130 items (at 50 items/min)\n",
      "2022-10-14 17:50:04 [scrapy.extensions.logstats] INFO: Crawled 192 pages (at 49 pages/min), scraped 175 items (at 45 items/min)\n",
      "2022-10-14 17:51:04 [scrapy.extensions.logstats] INFO: Crawled 244 pages (at 52 pages/min), scraped 223 items (at 48 items/min)\n",
      "2022-10-14 17:52:04 [scrapy.extensions.logstats] INFO: Crawled 295 pages (at 51 pages/min), scraped 270 items (at 47 items/min)\n",
      "2022-10-14 17:53:04 [scrapy.extensions.logstats] INFO: Crawled 347 pages (at 52 pages/min), scraped 318 items (at 48 items/min)\n",
      "2022-10-14 17:54:04 [scrapy.extensions.logstats] INFO: Crawled 402 pages (at 55 pages/min), scraped 369 items (at 51 items/min)\n",
      "2022-10-14 17:55:04 [scrapy.extensions.logstats] INFO: Crawled 450 pages (at 48 pages/min), scraped 414 items (at 45 items/min)\n",
      "2022-10-14 17:56:04 [scrapy.extensions.logstats] INFO: Crawled 503 pages (at 53 pages/min), scraped 463 items (at 49 items/min)\n",
      "2022-10-14 17:57:04 [scrapy.extensions.logstats] INFO: Crawled 552 pages (at 49 pages/min), scraped 508 items (at 45 items/min)\n",
      "2022-10-14 17:58:04 [scrapy.extensions.logstats] INFO: Crawled 606 pages (at 54 pages/min), scraped 558 items (at 50 items/min)\n",
      "2022-10-14 17:59:04 [scrapy.extensions.logstats] INFO: Crawled 658 pages (at 52 pages/min), scraped 606 items (at 48 items/min)\n",
      "2022-10-14 18:00:04 [scrapy.extensions.logstats] INFO: Crawled 703 pages (at 45 pages/min), scraped 648 items (at 42 items/min)\n",
      "2022-10-14 18:01:04 [scrapy.extensions.logstats] INFO: Crawled 758 pages (at 55 pages/min), scraped 699 items (at 51 items/min)\n",
      "2022-10-14 18:02:04 [scrapy.extensions.logstats] INFO: Crawled 811 pages (at 53 pages/min), scraped 748 items (at 49 items/min)\n",
      "2022-10-14 18:03:04 [scrapy.extensions.logstats] INFO: Crawled 867 pages (at 56 pages/min), scraped 800 items (at 52 items/min)\n",
      "2022-10-14 18:04:04 [scrapy.extensions.logstats] INFO: Crawled 916 pages (at 49 pages/min), scraped 846 items (at 46 items/min)\n",
      "2022-10-14 18:04:39 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-10-14 18:04:39 [scrapy.extensions.feedexport] INFO: Stored csv feed (875 items) in: results/hostels_booking2.csv\n",
      "2022-10-14 18:04:39 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 1196784,\n",
      " 'downloader/request_count': 949,\n",
      " 'downloader/request_method_count/GET': 949,\n",
      " 'downloader/response_bytes': 205287696,\n",
      " 'downloader/response_count': 949,\n",
      " 'downloader/response_status_count/200': 946,\n",
      " 'downloader/response_status_count/301': 1,\n",
      " 'downloader/response_status_count/302': 1,\n",
      " 'downloader/response_status_count/500': 1,\n",
      " 'elapsed_time_seconds': 1114.981583,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 10, 14, 16, 4, 39, 166870),\n",
      " 'httpcompression/response_bytes': 1196319320,\n",
      " 'httpcompression/response_count': 946,\n",
      " 'item_scraped_count': 875,\n",
      " 'log_count/INFO': 29,\n",
      " 'request_depth_max': 71,\n",
      " 'response_received_count': 946,\n",
      " 'retry/count': 1,\n",
      " 'retry/reason_count/500 Internal Server Error': 1,\n",
      " 'scheduler/dequeued': 949,\n",
      " 'scheduler/dequeued/memory': 949,\n",
      " 'scheduler/enqueued': 949,\n",
      " 'scheduler/enqueued/memory': 949,\n",
      " 'start_time': datetime.datetime(2022, 10, 14, 15, 46, 4, 185287)}\n",
      "2022-10-14 18:04:39 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "process.crawl(BookingSpider)\n",
    "process.start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1cca9558bc5ad879ec93cc030b157d75f18267527c60932cecaace349eef54dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
